name: pr-workflow

on:
  pull_request:
    branches:
      - development
  workflow_dispatch:

jobs:
  build-us-west-2:
    runs-on: ubuntu-latest
    env:
      REGION: "us-west-2"
      VERSION: "2.0.5"
      EMAIL: ${{ secrets.INVITATION_EMAIL_RECIPIENT }}
    steps:
      - name: Check out pr branch
        uses: actions/checkout@v2
        with:
          ref: ${{ github.sha }}

      - name: Initialize AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.BUILD_AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.BUILD_AWS_SECRET_ACCESS_KEY }}
          aws-region: us-west-2

      - name: Setup build environment
        run: |
          echo "SHORT_SHA=`git rev-parse --short HEAD`" >> $GITHUB_ENV
          DATETIME=$(date '+%s')
          echo "DIST_OUTPUT_BUCKET=media-insights-engine-frontend-$DATETIME" >> $GITHUB_ENV

      - name: Run build script
        run: |
          cd deployment
          aws s3 mb s3://$DIST_OUTPUT_BUCKET-$REGION --region $REGION
          ./build.sh ${DIST_OUTPUT_BUCKET}-${REGION} ${VERSION} ${REGION} | tee >( awk '/Without existing MIE deployment/{getline; print}' >template )

      - name: Deploy stack
        run: |
          cd deployment
          TEMPLATE=$(cat template | cut -f 2 -d "'")
          rm -f template
          STACK_NAME="pr${SHORT_SHA}"
          set -x

          # If STACK_NAME already exists then delete it.
          # This is necessary in order to rerun github action workflows.
          aws cloudformation describe-stacks --stack-name $STACK_NAME --region $REGION 2>&1 | grep "does not exist" > /dev/null
          if [ $? -ne 0 ]; then
            echo "$STACK_NAME stack already exists."
            echo "Removing stack $STACK_NAME";
            aws cloudformation delete-stack --stack-name $STACK_NAME --region $REGION
            aws cloudformation wait stack-delete-complete --stack-name $STACK_NAME
          fi

          aws cloudformation create-stack --stack-name $STACK_NAME --template-url $TEMPLATE --region $REGION --parameters ParameterKey=AdminEmail,ParameterValue=$EMAIL --capabilities CAPABILITY_IAM CAPABILITY_NAMED_IAM CAPABILITY_AUTO_EXPAND --disable-rollback
          aws cloudformation wait stack-create-complete --stack-name $STACK_NAME
          set +x
          WEBAPP_URL=$(aws cloudformation --region us-west-2 describe-stacks --stack-name $STACK_NAME --query "Stacks[0].Outputs[?OutputKey=='ContentAnalyisSolution'].OutputValue" --output text)
          # Make the WEBAPP_URL available to the next workflow step
          echo "WEBAPP_URL=${WEBAPP_URL}" >> $GITHUB_ENV

      - name: Clean build environment
        run: aws s3 rb s3://${DIST_OUTPUT_BUCKET}-${REGION} --force

      - name: Get login credentials
        run: |
          # Iterate thru all the files in the s3://github-test-bot2 until you find the invitation email that references our stack
          NUM_EMAILS=$(aws s3 ls s3://github-test-bot2 | wc -l)
          for i in `seq 1 $NUM_EMAILS`; do
            INVITATION_EMAIL=$(aws s3api list-objects-v2 --bucket "github-test-bot2" --query 'reverse(sort_by(Contents, &LastModified))['$((i-1))'].Key' --output=text)
            # Make sure it belongs to our stack
            aws s3 cp s3://github-test-bot2/$INVITATION_EMAIL ./invitation_email --quiet
            STACK_NAME="pr${SHORT_SHA}"
            # Check to see if this invitation email is for the $STACK_NAME stack
            grep ":stack/${STACK_NAME}" ./invitation_email > /dev/null
            if [ $? -eq 0 ];
              then
              echo "Found invitation email in s3://github-test-bot2/$INVITATION_EMAIL"
              # we found the invitation email so quit looking
              break;
            fi;
          done;
          # Remove the invitation email from s3
          aws s3 rm s3://github-test-bot2/$INVITATION_EMAIL
          TEMP_PASSWORD=$(cat ./invitation_email | grep 'temporary password' | sed 's/.*password is \(.*\)<br>AWS.*/\1/')
          # Password may contain HTML entities, so decode them to characters
          TEMP_PASSWORD=$(echo $TEMP_PASSWORD | perl -MHTML::Entities -pe 'decode_entities($_);')
          # Make TEMP_PASSWORD available to the next workflow step
          echo "TEMP_PASSWORD=${TEMP_PASSWORD}" >> $GITHUB_ENV

      - name: Start workflow
        run: |
          STACK_NAME="pr${SHORT_SHA}"
          # Get the workflow api endpoint
          MIE_STACK_NAME=$(aws cloudformation list-stacks --query 'StackSummaries[?starts_with(StackName,`'$STACK_NAME'-MieStack`)].StackName' --output json --region $REGION | grep MieStack | cut -f 2 -d '"' | tail -n 1)
          WORKFLOW_API_ENDPOINT=$(aws cloudformation describe-stacks --stack-name "$MIE_STACK_NAME" --region $REGION --query "Stacks[0].Outputs[?OutputKey=='WorkflowApiEndpoint'].OutputValue" --output text)
          DATAPLANE_BUCKET=$(aws cloudformation describe-stacks --stack-name "$MIE_STACK_NAME" --region $REGION --query "Stacks[0].Outputs[?OutputKey=='DataplaneBucket'].OutputValue" --output text)
          # Upload a test video file
          wget -q https://techmkt-videoarchive.s3-us-west-2.amazonaws.com/amazon_studios/sizzle_reels/Amazon+TCA+2019+Series+Sizzle.mp4
          aws s3 cp Amazon+TCA+2019+Series+Sizzle.mp4 s3://${DATAPLANE_BUCKET}
          # Install an IAM enabled HTTP client
          pip install awscurl

          #
          # Uncomment to enable CasImageWorkflow:
          #
          # #####################################
          # ###### TEST CasImageWorkflow  #######
          # #####################################
          # # TODO: upload TEST_IMAGE.png image file to dataplane bucket
          # # Get workflow configuration
          # WORKFLOW_NAME=CasImageWorkflow
          # # Disable faceSearch
          # WORKFLOW_CONFIGURATION=$(awscurl -X GET --region us-west-2 ${WORKFLOW_API_ENDPOINT}workflow/$WORKFLOW_NAME | cut -f 2 -d "'" | perl -pe 's/"Definition.+?}]}}}",//g' | jq '.Stages.RekognitionStage.Configuration' --compact-output)
          # WORKFLOW_CONFIGURATION=$(echo $WORKFLOW_CONFIGURATION | sed -e 's/"faceSearchImage":{"MediaType":"Image","Enabled":true}/"faceSearchImage":{"MediaType":"Image","Enabled":false}/')
          # WORKFLOW_CONFIGURATION='{"RekognitionStage":'$WORKFLOW_CONFIGURATION'}'
          # # Execute workflow
          # awscurl -X POST --region us-west-2 --data '{"Name":"CasImageWorkflow", "Configuration":'$WORKFLOW_CONFIGURATION', "Input":{"Media":{"Image":{"S3Bucket": "'${DATAPLANE_BUCKET}'", "S3Key":"TEST_IMAGE.png"}}}}' ${WORKFLOW_API_ENDPOINT}workflow/execution > curl.txt

          # # Wait until the workflow is done
          # WORKFLOW_ID=$(cat curl.txt | cut -f 2 -d "'" | perl -pe 's/"Definition.+?}]}}}",//g' | jq '.Id' --raw-output)
          # WORKFLOW_STATUS=$(awscurl -X GET --region us-west-2 ${WORKFLOW_API_ENDPOINT}workflow/execution/${WORKFLOW_ID} | cat | cut -f 2 -d "'" | perl -pe 's/"Definition.+?}]}}}",//g' | jq '.Status' --raw-output)
          # while [ "$WORKFLOW_STATUS" = "Started" ] || [ "$WORKFLOW_STATUS" = "Queued" ]; do sleep 1; WORKFLOW_STATUS=$(awscurl -X GET --region us-west-2 ${WORKFLOW_API_ENDPOINT}workflow/execution/${WORKFLOW_ID} | cat | cut -f 2 -d "'" | perl -pe 's/"Definition.+?}]}}}",//g' | jq '.Status' --raw-output); echo $WORKFLOW_STATUS; done

          ####################################
          ###### TEST CasVideoWorkflow #######
          ####################################
          WORKFLOW_NAME=CasVideoWorkflow
          # Disable faceSearch and GenericDataLookup operator
          WORKFLOW_CONFIGURATION=$(awscurl -X GET --region us-west-2 ${WORKFLOW_API_ENDPOINT}workflow/$WORKFLOW_NAME | cut -f 2 -d "'" | perl -pe 's/"Definition.+?}]}}}",//g' | jq '.Stages.defaultVideoStage.Configuration' --compact-output)
          WORKFLOW_CONFIGURATION=$(echo $WORKFLOW_CONFIGURATION | sed -e 's/"faceSearch":{"MediaType":"Video","Enabled":true}/"faceSearch":{"MediaType":"Video","Enabled":false}/')
          WORKFLOW_CONFIGURATION=$(echo $WORKFLOW_CONFIGURATION | sed -e 's/"GenericDataLookup":{"MediaType":"Video","Enabled":true}/"GenericDataLookup":{"MediaType":"Video","Enabled":false}/')
          WORKFLOW_CONFIGURATION='{"defaultVideoStage":'$WORKFLOW_CONFIGURATION'}'
          echo "WORKFLOW_CONFIGURATION:"
          echo $WORKFLOW_CONFIGURATION
          echo "Starting CasVideoWorkflow"
          set -x
          # Execute workflow
          awscurl -X POST --region us-west-2 --data '{"Name":"CasVideoWorkflow", "Configuration":'$WORKFLOW_CONFIGURATION', "Input":{"Media":{"Video":{"S3Bucket": "'${DATAPLANE_BUCKET}'", "S3Key":"Amazon+TCA+2019+Series+Sizzle.mp4"}}}}' ${WORKFLOW_API_ENDPOINT}workflow/execution > curl.txt
          set +x
          # Wait until the workflow is done
          WORKFLOW_ID=$(cat curl.txt | cut -f 2 -d "'" | perl -pe 's/"Definition.+?}]}}}",//g' | jq '.Id' --raw-output)
          echo "WORKFLOW_ID: $WORKFLOW_ID"
          WORKFLOW_STATUS=$(awscurl -X GET --region us-west-2 ${WORKFLOW_API_ENDPOINT}workflow/execution/${WORKFLOW_ID} | cat | cut -f 2 -d "'" | perl -pe 's/"Definition.+?}]}}}",//g' | jq '.Status' --raw-output)
          echo "Waiting for workflow to complete..."
          while [ "$WORKFLOW_STATUS" = "Started" ] || [ "$WORKFLOW_STATUS" = "Queued" ]; do sleep 10; WORKFLOW_STATUS=$(awscurl -X GET --region us-west-2 ${WORKFLOW_API_ENDPOINT}workflow/execution/${WORKFLOW_ID} | cat | cut -f 2 -d "'" | perl -pe 's/"Definition.+?}]}}}",//g' | jq '.Status' --raw-output); echo -n '.'; done
          echo -e "\n$WORKFLOW_STATUS"

      - name: Start puppeteer
        # The checkout action above changes the work dir in a way that breaks
        # the docker commands in this action, so we need to specify work dir
        # explicitly here.
        working-directory: /home/runner/work/
        run: |
          echo "Downloading puppeteer tests"
          git clone https://github.com/iandow/aws-media-insights-puppeteer
          cd aws-media-insights-puppeteer
          echo "Building puppeteer tests"
          npm i puppeteer puppeteer-screenshot-tester --quiet
          docker build --tag=cas-puppeteer:latest . --quiet
          echo "Running puppeteer tests"
          docker run --rm -v "$PWD":/usr/src/app -e WEBAPP_URL="${{ env.WEBAPP_URL }}" -e TEMP_PASSWORD="${{ env.TEMP_PASSWORD }}" cas-puppeteer:latest

#      - name: Clean the test environment
#        run: |
#          STACK_NAME="pr${SHORT_SHA}"
#          echo "Removing $STACK_NAME"
#          aws cloudformation delete-stack --stack-name $STACK_NAME --region $REGION
#          echo "Waiting for stack to delete..."
#          aws cloudformation wait stack-delete-complete --stack-name $STACK_NAME
#          echo "Removing $STACK_NAME S3 buckets:"
#          aws s3 ls | awk '{print $3}' | grep $STACK_NAME  | while read line; do
#          echo "s3://$line;
#          aws s3 rb s3://{} --force > /dev/null;
#          done
